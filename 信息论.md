# 信息论

信息论是应用数学的一个分支，主要研究的是对一个信号包含信息的多少进行量化。**信息论的基本思想是一个不太可能的事件居然发生了，要比一个非常可能的事件发生，能提供更多的信息**。比如，“今天早上有日食”比"今天早上太阳升起"的信息量更多。

## 自信息

信息的量度应该依赖于概率分布$P(x)$，因此我们想要寻找一个函数$I(x)$，它是概率的单调函数，来表达信息的内容。怎么寻找呢？如果我们有两个不相关的事件$x$和$y$，那么观察两个事件同时发生时获得的信息量应该等于观察到事件各自发生时获得的信息之和，$T(x,y)=I(x)+I(y)$。因为两个事件是独立的，因此$P(x,y)=p(x)p(y)$。我们很容易想到对数，对于信息，我们有如下假定：

- 非常可能发生的事件信息量要比较少，并且极端情况下，确保能够发生的事件应该没有信息量
- 较不可能发生的事件具有更高的信息量
- 独立事件应具有增量的信息

为了满足上述3个性质，我们定义一个事件$X=x$的**自信息**为

$$I(x)=-\log{P(x)}$$

其中$P(x)$表示事件$x$发生的概率，若自信息的底数为$e$则单位是**奈特**，底数为2，则单位是**比特**或**香农**。事件发生的概率越大，则自信息的值越小，事件发生带来的信息量越少。

## 熵

自信息刻画的是单个事件的信息量，而对于**整个概率分布中的不确定性总量**，我们可以用**熵**来进行量化：

$$H(X)=-\sum_{i=1}^{n}{p_i\log{p_i}}​$$

也就是说，一个分布的**熵**是指遵循这个分布的事件所产生的期望信息量。熵越大，随机变量的不确定性就越大。已2为底数的熵叫做香农熵。

## 条件熵

设有随机变量$(X,Y)​$，其联合概率分布为：

$$P(X=x_i, Y=y_i)=p_{ij},\ i=1,2,3,...,n;\ j=1,2,...,m$$

条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。随机变量$X$给定的条件下随机变量$Y$的条件熵$H(Y|X)$，定义为$X$给定下$Y$的条件概率分布的熵对$X$的数学期望

$$H(Y|X)=\sum_{i=1}^{n}{p_iH(Y|X=x_i)}$$

该公式计算X取不同值时，随机变量分布的期望

## 相对熵

相对熵也叫KL散度，如果对于同一个随机变量$x$有两个单独的概率分布$p(x)$和$q(x)$，可以使用**KL散度**来衡量这两个分布的差异，相对熵越大，两个分布差异越大，则$p$对$q$的相对熵：

$$D_{KL}(p||q)=\displaystyle\sum_{x}p(x)\log\frac{p(x)}{q(x)}=E_{p(x)}\log\frac{p(x)}{q(x)}$$

1、如果$p(x)$和$q(x)$两个分布相同，那么相对熵等于0

2、$D_{KL}(p||q)\not=D_{KL}(q||p)​$ ，相对熵具有不对称性。大家可以举个简单例子算一下。

3、$D_{KL}(p||q)\geq0$

## 交叉熵

现在有关于样本集的两个概率分布$p(x)$和$q(x)$，其中$p(x)$为真实分布，$q(x)$预测分布。如果用真实分布$p(x)$来衡量识别别一个样本所需要编码长度的期望（平均编码长度）为:

$$H(p) =\displaystyle\sum_{x}p(x)\log\frac{1}{p(x)}$$

如果使用预测分布$q(x)$来表示来自真实分布$p(x)$的平均编码长度，则是：

$$H(p,q)=\displaystyle\sum _{x}p(x)\log\frac{1}{q(x)}$$

因为用$q(x)$来编码的样本来自于分布$q(x)$，所以$H(p,q)$中的概率是$p(x)$。此时就将$H(p,q)$称之为**交叉熵**。

我们化简相对熵的公式：

$$D_{KL}(p||q)=\displaystyle\sum_{x}p(x)\log\frac{p(x)}{q(x)}=\sum_{x}p(x)\log p(x)-p(x)logq(x)$$

可知$D_{KL}(p||q)=H(p,q)-H(q)$，即**相对熵等于交叉熵和熵的差**。

## 信息增益

信息增益表示得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度。

# test

# 参考

[[详解机器学习中的熵、条件熵、相对熵和交叉熵](https://www.cnblogs.com/kyrieng/p/8694705.html)](https://www.cnblogs.com/kyrieng/p/8694705.html)