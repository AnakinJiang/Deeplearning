# 信息论

信息论是应用数学的一个分支，主要研究的是对一个信号包含信息的多少进行量化。信息论的基本思想是一个不太可能的事件居然发生了，要比一个非常可能的事件发生，能提供更多的信息。比如，"今天早上太阳升起"比“今天早上有日食”的信息量更多。

## 自信息

对于信息，我们有如下假定：

- 非常可能发生的事件信息量要比较少，并且极端情况下，确保能够发生的事件应该没有信息量
- 较不可能发生的事件具有更高的信息量
- 独立事件应具有增量的信息

为了满足上述3个性质，我们定义一个事件$X=x$的**自信息**为

$$I(x)=-\log{P(x)}$$

若自信息的底数为$e$则单位是**奈特**，底数为2，则单位是**比特**或**香农**。事件发生的概率越大，则自信息的值越小，事件发生带来的信息量越少。

## 熵

自信息刻画的是单个时间的信息量，而对于整个概率分布中的不确定性总量，我们可以用**熵**来进行量化：

$$H(X)=-\sum_{i=1}^{n}{p_i\log{p_i}}​$$

也就是说，一个分布的**熵**是指遵循这个分布的事件所产生的期望信息量。熵越大，随机变量的不确定性就越大。已2为底数的熵叫做香农熵。

## 条件熵

设有随机变量$(X,Y)​$，其联合概率分布为：

$$P(X=x_i, Y=y_i)=p_{ij},\ i=1,2,3,...,n;\ j=1,2,...,m$$

条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。随机变量$X$给定的条件下随机变量$Y$的条件熵$H(Y|X)$，定义为$X$给定下$Y$的条件概率分布的熵对$X$的数学期望

$$H(Y|X)=\sum_{i=1}^{n}{p_iH(Y|X=x_i)}$$

该公式计算X取不同值时，随机变量分布的期望

## 信息增益

信息增益表示得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度。

# test