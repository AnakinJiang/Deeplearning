[TOC]

[上一章](01线性回归与梯度下降.md)，我们介绍了线性回归、梯度下降及最小二乘算法。这一章，我们介绍梯度下降各个参数的调整策略以及线性回归损失的概率解释。

# 1、参数调整

## 1.1、归一化

在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。

以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能。如下左图所示，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。

![](images/02/ml_02_2.png)

解决的方法是**归一化**，即尝试将所有特征的尺度都尽量缩放到-1到1之间。如上右图所示。常见的归一化方法有：

**线性归一化**

$$x'=\frac{x-min(x)}{max(x)-min(x)}$$

这种归一化将原始数据转换为$[0\ 1]$的范围，但是有也有一个缺陷，如果$max$和$min$不稳定，很容易使归一化的结果不稳定，故常常可以用经验常量值来替代。

**0均值标准化**

$$x'=\frac{{{x}_{n}}-{{\mu}_{n}}}{{{s}_{n}}}$$

其中 ${\mu_{n}}$是平均值，${s_{n}}$是标准差。

## 1.2、学习率选择

梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，我们可以绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛。

![](images/02/ml_02_3.jpg)

也有一些自动测试是否收敛的方法，例如将代价函数的变化值与某个阀值（例如0.001）进行比较，但通常看上面这样的图表更好。

梯度下降算法的每次迭代受到学习率的影响，如果学习率$a$过小，则达到收敛所需的迭代次数会非常高；如果学习率$a$过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。

通常可以考虑尝试些学习率：

$\alpha=0.01，0.03，0.1，0.3，1，3，10$

## 1.3、特征和多项式回归

如房价预测问题，

![](images/02/ml_02_4.png)

$h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}\times{frontage}+{\theta_{2}}\times{depth}$ 

${x_{1}}=frontage$（临街宽度），${x_{2}}=depth$（纵向深度），$x=frontage*depth=area$（面积），则：${h_{\theta}}\left( x \right)={\theta_{0}}+{\theta_{1}}x$。
线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，比如一个二次方模型：$h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}^2}$
 或者三次方模型： $h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}^2}+{\theta_{3}}{x_{3}^3}$ 

![](images/02/ml_02_5.jpg)

通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外，我们可以令：

${{x}_{2}}=x_{2}^{2},{{x}_{3}}=x_{3}^{3}$，从而将模型转化为线性回归模型。

根据函数图形特性，我们还可以使：

${{{h}}_{\theta}}(x)={{\theta }_{0}}\text{+}{{\theta }_{1}}(size)+{{\theta}_{1}}{{(size)}^{2}}$

或者:

${{{h}}_{\theta}}(x)={{\theta }_{0}}\text{+}{{\theta }_{1}}(size)+{{\theta }_{1}}\sqrt{size}$

注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。

## 1.4、代码实现

```matlab
%% 线性回归
% 数据来源来自吴恩达机器学习Couresa第一次作业

data = load("Data/ex1data2.txt");
x = data(:, 1:end-1);
y = data(:, end);
m = size(x, 1);
n = size(x, 2);
theta = zeros(1, n + 1);
iteration = 400;
alpha = 0.01;
J_history = zeros(iteration, 1);
mu = mean(x);
% 特征缩放
sigma = std(x);
X_norm = (x - mu)./sigma;
X_norm = [ones(m, 1), X_norm];
for i = 1:iteration
    theta = theta - alpha*(X_norm*theta'-y)'*X_norm/m;
    J_history(i) = sum((X_norm*theta'-y).^2)/2/m;
end
plot(1:numel(J_history), J_history, '-b', 'LineWidth', 2);
disp(theta);
disp(J);
```

实现了对多变量线性回归的特征缩放。

# 2、概率解释（Probabilistic interpretation）

在面对回归问题的时候，可能有这样一些疑问，就是为什么选择线性回归，尤其是为什么而是最小二乘法成本函数 J ？在本节里，我们会给出一系列的概率基本假设，基于这些假设，就可以推出最小二乘法回归是一种非常自然的算法。
首先咱们假设目标变量和输入值存在下面这种等量关系：

$ y^{(i)}=\theta^T x^{(i)}+ \epsilon ^{(i)}$

上式中 $ \epsilon ^{(i)}$ 是误差项，用于存放由于建模所忽略的变量导致的效果 (比如可能某些特征对于房价的影响很明显，但我们做回归的时候忽略掉了)或者随机的噪音信息（random noise）。进一步假设 $ \epsilon ^{(i)}$   是独立同分布的 (IID ，independently and identically distributed) ，服从高斯分布（Gaussian distribution ，也叫正态分布 Normal distribution），其平均值为 0，方差（variance）为 $\sigma ^2$。这样就可以把这个假设写成 $ \epsilon ^{(i)} ∼ N (0, \sigma ^2)$ 。然后 $ \epsilon ^{(i)} $  的密度函数就是：

$ p(\epsilon ^{(i)} )= \frac 1{\sqrt{2\pi\sigma}} exp (- \frac  {(\epsilon ^{(i)} )^2}{2\sigma^2})$

这意味着存在下面的等量关系：

$ p(y ^{(i)} |x^{(i)}; \theta)= \frac 1{\sqrt{2\pi\sigma}} exp (- \frac  {(y^{(i)} -\theta^T x ^{(i)} )^2}{2\sigma^2})$

这里的记号 $ p(y ^{(i)} |x^{(i)}; \theta)$ 表示的是这是一个对于给定 $x^{(i)}$ 的 $y^{(i)}$ 的分布，用θ 进行了参数化。 

注意这里咱们不能用 $\theta (p(y ^{(i)} |x^{(i)},\theta))$来当做条件，因为 θ 并不是一个随机变量。这个 $y^{(i)}$  的分布还可以写成$y^{(i)} | x^{(i)}; \theta ∼ N (\theta ^T x^{(i)}, \sigma^2)$。
给定一个 x 为设计矩阵（design matrix），包含了全部$x^{(i)}$，然后再给定 θ，那么 $y^{(i)}$ 的分布是什么？数据的概率以$p (\vec{y}|X;\theta )$ 的形式给出。在θ取某个固定值的情况下，这个等式通常可以看做是一个 $\vec{y}$ 的函数（也可以看成是 x 的函数）。当我们要把它当做 θ 的函数的时候，就称它为 **似然函数（likelihood function）**

$
\begin{aligned}
L(\theta) &=L(\theta;X,\vec{y})\\
&=p(\vec{y}|X;\theta)\\
\end{aligned}
$

结合之前对 $\epsilon^{(i)}$ 的独立性假设 (这里对$y^{(i)}$ 以及给定的 $x^{(i)}$ 也都做同样假设)，就可以把上面这个等式改写成下面的形式：

$
\begin{aligned}
L(\theta) &=\prod ^m _{i=1}p(\vec{y}|X;\theta)\\
&=\prod ^m _{i=1} \frac  1{\sqrt {2\pi \sigma }} exp(- \frac {(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2})\\
\end{aligned}
$



现在，给定了$y^{(i)})$ 和 $x^{(i)})$之间关系的概率模型了，用什么方法来选择咱们对参数 θ 的最佳猜测呢？最大似然法（maximum likelihood）告诉我们要选择能让数据的似然函数尽可能大的 θ。也就是说，咱们要找的 θ 能够让函数 $L(\theta)$ 取到最大值。
除了找到 $L(\theta)$ 最大值，我们还以对任何严格递增的 $L(\theta)$ 的函数求最大值。如果我们不直接使用 $L(\theta)$，而是使用对数函数，来找对数函数 $l(\theta)$ 的最大值，那这样对于求导来说就简单了一些：

$\begin{aligned}
l(\theta) &=\log L(\theta)\\
&=\log \prod ^m _{i=1} \frac  1{\sqrt {2\pi \sigma }} exp(- \frac {(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2})\\
&= \sum ^m _{i=1} \log (\frac  1{\sqrt {2\pi \sigma }} exp(- \frac {(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2}))\\
&= m \times \log \frac  1{\sqrt {2\pi \sigma}}- \frac 1{\sigma^2}\times \frac 12 \sum^m_{i=1} (y^{(i)}-\theta^Tx^{(i)})^2\\
\end{aligned}
$

因此，对 $l(\theta)$ 取得最大值也就意味着下面这个子式取到最小值：

$ \frac 12 \sum^m _{i=1} (y^{(i)}-\theta^Tx^{(i)})^2$

到这里我们能发现这个子式实际上就是 $J(\theta)$，也就是最原始的最小二乘成本函数（least-squares cost function）。总结一下也就是：在对数据进行概率假设的基础上，最小二乘回归得到的 θ 和最大似然法估计的 θ 是一致的。所以这是一系列的假设，其前提是认为最小二乘回归（least-squares regression）能够被判定为一种非常自然的方法，这种方法正好就进行了最大似然估计（maximum likelihood estimation）。（要注意，对于验证最小二乘法是否为一个良好并且合理的过程来说，这些概率假设并不是必须的，此外可能（也确实）有其他的自然假设能够用来评判最小二乘方法。）
另外还要注意，在刚才的讨论中，我们最终对 θ 的选择并不依赖 $\sigma^2$，而且也确实在不知道 $\sigma^2$ 的情况下就已经找到了结果。稍后我们还要对这个情况加以利用，到时候我们会讨论指数族以及广义线性模型。

# 3、梯度下降法和最小二乘法的区别

我们发现线性回归的损失函数和最小二乘法十分相似，这里我们补充一下最小二乘法的相关概念，并将其和梯度下降法作比较。

## 3.1、最小二乘法

最小二乘法是一种优化算法，通过最小化误差的平方和寻找数据最佳函数匹配，可以用于曲线拟合。

我们以最简单的一元线性模型来解释最小二乘法。对于一元线性回归模型, 假设从总体中获取了n组观察值（X1，Y1），（X2，Y2）， …，（Xn，Yn）。对于平面中的这n个点，可以使用无数条曲线来拟合。要求样本回归函数尽可能好地拟合这组值。综合起来看，这条直线处于样本数据的中心位置最合理。 选择最佳拟合曲线的标准可以确定为：使总的拟合误差（即总残差）达到最小。有以下三个标准可以选择：

1. 用“残差和最小”确定直线位置是一个途径。但很快发现计算“残差和”存在相互抵消的问题
2. 用“残差绝对值和最小”确定直线位置也是一个途径。但绝对值的计算比较麻烦
3. 最小二乘法的原则是以“残差平方和最小”确定直线位置。用最小二乘法除了计算比较方便外，得到的估计量还具有优良特性。这种方法对异常值非常敏感。

最常用的是普通最小二乘法（ Ordinary  Least Square，OLS）：所选择的回归模型应该使所有观察值的残差平方和达到最小。（Q为残差平方和）- 即采用平方损失函数。

## 3.2、最小二乘法与梯度下降的异同

这两种方法的目的相同，并且对于损失函数的定义都是相同的--求得损失函数的最小值，使得假设函数能够更好的拟合训练集数据。但是明显也是有区别的：

计算上，最小二乘法直接计算损失函数的极值，而梯度下降却是给定初始值，按照梯度一步步下降的方式取得局部最小值，之后再选定其他初始值，计算-比较。

数学上，最小二乘法直接使用极值，将极值作为最小值。其假定有二：1，损失函数中极值就是最小值。2，损失函数具有极值。而梯度下降则不同，梯度下降并没有什么假定，是利用函数中某一点的梯度，一步步寻找到损失函数的局部最小值，之后对多个局部最小值进行比较(选定不同的初始值)，确定全局最小值。

总体来说：最小二乘法计算简单，但梯度下降法更加通用！