# 极大似然估计

## 极大似然原理



# 联合概率分布

# 拉格郎日对偶性

## 原始问题

假设$f(x)$，$c_i(x)$，$h_j(x)$是定义在$R^n$上的连续可谓函数，考虑约束最优化问题：
$$
\begin{aligned}
min_w & f(w)& \\
s.t.  g_i(w)& =\gamma,i=1,...,k\\
h_i(w)& =\gamma,i=1,...,l\\
\end{aligned}
$$
要解决上面这样的问题，首先要定义**广义拉格朗日函数(generalized Lagrang_ian)：**
$$
L(w,\alpha,\beta)=f(w)+\sum^k_{i=1}\alpha_ig_i(w)+\sum^l_{i=1}\beta_ih_i(w)
$$
上面的式子中，$\alpha_i$和$\beta_i$都是**拉格朗日乘数(Lagrange multipliers)**。设有下面这样一个量(quantity)：
$$
\theta_{P}(w)=\max_{\alpha,\beta:\alpha_i \geq 0}L(w,\alpha,\beta)
$$
上式中的 “P” 是对 “primal” 的简写。设已经给定了某些 w。如果 w 不能满足某些主要约束，(例如对于某些 i 存在 $g_i(w) > 0$ 或者 ​$h_i(w) \neq 0$)，那么咱们就能够证明存在下面的等式关系：
$$
\begin{aligned}
\theta_P(w)&=\max_{\alpha,\beta:\alpha_i \geq 0} f(w)+\sum^k_{i=1}\alpha_ig_i(w)+\sum^l_{i=1}\beta_ih_i(w) &\text{(1)}\\
&= \infty &\text{(2)}\\
\end{aligned}
$$
与之相反，如果 w 的某些特定值确实能满足约束条件，那么则有 $\theta_P(w) = f(w)$。因此总结一下就是：
$$
\theta_P(w)= \begin{cases} f(w) & \text {if w satisfies primal constra_ints} \\
\infty & \text{otherwise} \end{cases}
$$
因此，如果 w 的所有值都满足主要约束条件，那么$\theta_P$的值就等于此优化问题的目标量，而如果约束条件不能被满足，那 $\theta_P$的值就是正无穷了。所以，进一步就可以引出下面这个最小化问题(minimization problem)：
$$
\min_w \theta_P(w)=\min_w \max_{\alpha,\beta:\alpha_i\geq0} L(w,\alpha,\beta)
$$
这个新提出的问题与之前主要约束问题有一样的解，所以还是同一个问题。为了后面的一些内容，我们要在这里定义一个目标量的最优值(optimal value of the objective)$p \ast = min_w \theta_P (w)$；我们把这个值称为 主要优化问题的值(value of the primal problem)。

## 对偶问题

我们定义下面这个 $\theta_D$：
$$
\theta_D(\alpha,\beta)=\min_w L(w,\alpha,\beta)
$$
上面的式子中，“D” 是 “dual” 的缩写。这里要注意，在对$\theta_P$ 的定义中，之前是对 $\alpha$, $\beta$ 进行优化(找最大值)，这里则是找 w 的最小值。

现在我们就能给出这个对偶优化问题了：
$$
\max_{\alpha,\beta:\alpha_i\geq 0} \theta_D(\alpha,\beta)  = \max_{\alpha,\beta:\alpha_i\geq 0} \min_w L(w,\alpha,\beta)
$$
这个形式基本就和我们之前看到过的主要约束问题(primal problem)是一样的了，唯一不同是这里的“max” 和 “min” 互换了位置。我们也可以对这种对偶问题对象的最优值进行定义，即 $d\ast = max\alpha,\beta:\alpha_i\geq 0 \theta_D(w)$。

主要约束问题和这里的对偶性问题是怎么联系起来的呢？通过下面的关系就很容易发现
$$
d\ast = \max_{\alpha,\beta:\alpha_i\geq 0}\min_w L(w,\alpha,\beta) \leq \min_w \max_{\alpha,\beta:\alpha_i\geq 0}L(w,\alpha,\beta)  =p\ast
$$
**证明**

对任意$\alpha$，$\beta$和$x$，有
$$
\theta_D(\alpha,\beta)=\min_{x}{L(x,\alpha,\beta)}\le L(x,\alpha,\beta)\le \max_{\alpha,\beta;\alpha_i \ge0}{L(x,\alpha,\beta)}=\theta_P(x)
$$
即
$$
\theta_D(\alpha,\beta)\le\theta_P(x)
$$
由于原始问题和对偶问题均有最优值，所以
$$
\max_{\alpha,\beta;\alpha_i \ge0}{\theta_D(\alpha,\beta)}\le\theta_P(x)
$$
即
$$
d\ast = \max_{\alpha,\beta:\alpha_i\geq 0}\min_w L(w,\alpha,\beta) \leq \min_w \max_{\alpha,\beta:\alpha_i\geq 0}L(w,\alpha,\beta)  =p\ast
$$
接下来咱们就来看看导致上面二者相等的特定条件是什么。

假设 f 和 $g_i$ 都是凸的(convex$^3$)，$h_i$ 是仿射的(affine$^4$)。进一步设 g 是严格可行的(strictly feasible)；这就意味着会存在某个 w，使得对所有的 i 都有 $g_i(w) < 0$。

3 当 f 有一个海森矩阵(Hessian matrix)的时候，那么当且仅当这个海森矩阵(Hessian matrix)是半正定矩阵，f 才是凸的。例如，$f (w) = w^T w$ 就是凸的；类似的，所有的线性linear(以及仿射affine)函数也都是凸的。(不可微differentiable的函数 f 也可以是凸的，不过在哪买还不需要在这里对凸性进行那么泛化的扩展定义了。)

4 例如，存在 $a_i$和 $b_i$ 满足 $h_i(w) = a^T_i w + b_i$。仿射(Affine)的意思大概就跟线性 linear 差不多，不同的就是在矩阵进行了线性变换的基础上还增加了一个截距项(extra intercept term)$b_i$。

基于上面的假设，可知必然存在 $w\ast$，$\alpha\ast$， $\beta\ast$ 满足$w\ast$ 为主要约束问题(primal problem)的解，而$\alpha\ast$，$\beta\ast$ 为对偶问题的解，此外存在一个 $p\ast = d\ast = L(w\ast,\alpha\ast, \beta\ast)$。另外，$w\ast$，$\alpha\ast$， $\beta\ast$这三个还会满足卡罗需-库恩-塔克条件(Karush-Kuhn-Tucker conditions, 缩写为 KKT)，如下所示：
$$
\begin{aligned}
\frac{\partial}{\partial w_i}L(w\ast,\alpha\ast,\beta\ast) &= 0,i=1,...,n & \text{(3)}\\
\frac{\partial}{\partial \beta_i}L(w\ast,\alpha\ast,\beta\ast)&= 0 ,i=1,...,l &  \text{(4)}\\
\alpha_i\ast g_i(w\ast)&= 0,i=1,...,k & \text{(5)}\\
g_i(w\ast)&\leq 0,i=1,...,k & \text{(6)}\\
\alpha_i\ast &\geq 0,i=1,...,k &\text{(7)}\\
\end{aligned}
$$
反过来，如果某一组 $w\ast,\alpha\ast,\beta\ast$ 满足 KKT 条件，那么这一组值就也是主要约束问题(primal problem)和对偶问题的解。

这里咱们要注意一下等式(5)，这个等式也叫做 KKT 对偶互补条件(dual complementarity condition)。这个等式暗示，当$\alpha_i\ast \geq 0$ 的时候，则有 $g_i(w\ast) = 0$。(也就是说，$g_i(w) \leq 0$ 这个约束条件存在的话，则应该是相等关系，而不是不等关系。)后面的学习中，这个等式很重要，尤其对于表明 SVM 只有少数的“支持向量(Support Vectors)”；在学习 SMO 算法的时候，还可以用 KKT 对偶互补条件来进行收敛性检测(convergence test)。

# 贝叶斯定理

