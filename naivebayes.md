在高斯判别分析（GDA）方法中，特征向量 $x$ 是连续的，值为实数的向量。下面我们要讲的是当 $x_i$ 是离散值的时候来使用的另外一种学习算法。

本章将介绍朴素贝叶斯方法，首先介绍基本原理，它是基于贝叶斯定力与特征条件独立假设的分类方法。接着垃圾邮件分类的例子（伯努利朴素贝叶斯），然后介绍如何使用拉普拉斯光滑优化朴素贝叶斯，最后介绍SKlearn的代码实现。

# 基本原理

设输入空间$\mathcal{X}\subseteq R^n$为$n$维向量的集合，输出空间为类标记集合$\mathcal{Y}=\{c^{(1)},c^{(2)},...,c^{(K)}\}$。输入为特征向量$x\in\mathcal{X}$，输出为类标记$y\in\mathcal{Y}$，$X$是定义在输入空间$\mathcal{X}$上的随机向量，$Y$是定义在输出空间$\mathcal{Y}$上的随机变量。$P(X,Y)$是$X$和$Y$的联合概率分布。训练数据集
$$
T=\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(N)},y^{(N)})\}
$$
朴素贝叶斯法分类时，对给定的输入$x$，通过学习到的模型计算后验概率分布$P(Y=c_k|X=x)$，将后验概率最大的类作为$x$的类输出。后验概率计算根据贝叶斯定律计算：
$$
P(Y=c^{(k)}|X=x)=\frac{P(X=x|Y=c_k)P(Y=c^{(k)})}{\sum _{k}{P(X=x|Y=c^{(k)})P(Y=c^{(k)})}}
$$
对于先验概率分布
$$
P(Y=c^{(k)}),\ k=1,2,...,k
$$
我们利用极大似然估计法估计相应的概率，即根据训练集中各个类出现的次数估计整体先验概率，具体公式
$$
P(Y=c^{(k)})=\frac{\sum_{i=1}^{m}{}I(y^{(i)}=c^{(k)})}{m}, \ k=1,2,...,K
$$
对于条件概率分布
$$
P(X=x|Y=c^{(k)})=P(X_1=x_1,...,X_n=x_n|Y=c^{(k)}), \ k=1,2,...,K
$$
我们引入了条件独立性的假设，条件概率分布计算转换为
$$
\begin{align*}
P(X=x|Y=c^{(k)})&=P(X_1=x_1,...,X_n=x_n|Y=c^{(k)})\\
&=\prod_{j=1}^{n}P(X_j=x_j|Y=c^{(k)})
 \end{align*}
$$
将该公式带入后验概率公式，我们得到
$$
P(Y=c^{(k)}|X=x)=\frac{P(Y=c^{(k)})\prod_{j=1}^{n}P(X_j=x_j|Y=c^{(k)})}{\sum _{k}P(Y=c^{(k)}){\prod_{j=1}^{n}P(X_j=x_j|Y=c^{(k)})}}
$$
而对于第$j$个特征$x_j$可能取值的集合为$\{a_{j1},a_{j2},...,a_{jS_j}\} $，条件概率$p(x_j=a_{jl}|Y=c^{(k)})$的极大似然估计是
$$
P(X_j=a_{jl}|Y=c^{(k)})=\frac{\sum_{i=1}^{m}I(x^{(i)}_j=a_{jl},y^{(i)}=c^{(k)})}{\sum_{i=1}^{m}I(y^{(i)}=c^{(k)})}\\
j=1,2,...,n;l=1,2,...,S_j;k=1,2,...,K
$$
这样我们就能根据后验概率公式对输入的$x$进行分类，即
$$
y=f(x)=arg \max_{c^{(k)}}\frac{P(Y=c^{(k)})\prod_{j=1}^{n}P(X_j=x_j|Y=c^{(k)})}{\sum _{k}P(Y=c^{(k)}){\prod_{j=1}^{n}P(X_j=x_j|Y=c^{(k)})}}
$$
注意到对于输入的$x$， 其分母都是相同的，故
$$
y=f(x)=arg \max_{c^{(k)}}P(Y=c^{(k)})\prod_{j=1}^{n}P(X_j=x_j|Y=c^{(k)})
$$

# 垃圾邮件分类

我们我们首先以垃圾邮件分类为例子，讲解伯努利朴素贝叶斯。假设我们有了一个训练集（一堆已经标好了的是否为垃圾邮件的邮件）。要构建垃圾邮件分选器，咱们先要开始确定用来描述一封邮件的特征$x_i$有哪些。

我们将用一个特征向量来表示一封邮件，这个向量的长度等于字典中单词的个数。如果邮件中包含了字典中的第 i 个单词，那么就令 $x_i = 1$；反之则$x_i = 0$。例如下面这个向量：
$$
x=\begin{bmatrix}1\\0\\0\\\vdots \\1\\ \vdots \\0\end{bmatrix} \begin{matrix}\text{a}\\ \text{aardvark}\\ \text{aardwolf}\\ \vdots\\ \text{buy}\\ \vdots\\ \text{zygmurgy}\\ \end{matrix}
$$
就用来表示一个邮件，其中包含了两个单词 "a" 和 "buy"，但没有单词 "aardvark"， "aardwolf" 或者 "zymurgy"  。这个单词集合编码整理成的特征向量也成为词汇表（vocabulary,），所以特征向量 x 的维度就等于词汇表的长度。

> 注：实际应用中并不需要遍历整个英语词典来组成所有英语单词的列表，实践中更常用的方法是遍历一下训练集，然后把出现过一次以上的单词才编码成特征向量。这样做除了能够降低模型中单词表的长度之外，还能够降低运算量和空间占用，此外还有一个好处就是能够包含一些你的邮件中出现了而词典中没有的单词。有时候还要排除一些特别高频率的词汇，比如像冠词the，介词of 和and 等等；这些高频率但是没有具体意义的虚词也叫做stop words，因为很多文档中都要有这些词，用它们也基本不能用来判定一个邮件是否为垃圾邮件。

选好了特征向量了，接下来就是建立一个生成模型（generative model）。所以我们必须对$p(x|y)$进行建模。但是，假如我们的单词有五万个词，则特征向量$x \in  \{0, 1\}^{50000}$ (即 x是一个 50000 维的向量，其值是0或者1)，如果我们要对这样的 x进行多项式分布的建模，那么就可能有$2^{50000}$ 种可能的输出，然后就要用一个 $(2^{50000}-1)$维的参数向量。这样参数明显太多了。

要给$p(x|y)$建模，先来做一个非常强的假设。我们假设特征向量xi 对于给定的 y 是独立的。这个假设也叫做朴素贝叶斯假设（Naive Bayes ，NB assumption），基于此假设衍生的算法也就叫做朴素贝叶斯分选器（Naive Bayes classifier）。例如，如果 $y = 1$ 意味着一个邮件是垃圾邮件；然后其中"buy" 是第2087个单词，而 "price"是第39831个单词；那么接下来我们就假设，如果我告诉你 $y = 1$，也就是说某一个特定的邮件是垃圾邮件，那么对于$x_{2087}$ (也就是单词 buy 是否出现在邮件里)的了解并不会影响你对$x_{39831}$ (单词price出现的位置)的采信值。更正规一点，可以写成 $p(x_{2087}|y) = p(x_{2087}|y, x_{39831})$。（要注意这个并不是说$x_{2087}$ 和 $x_{39831}$这两个特征是独立的，那样就变成了$p(x_{2087}) = p(x_{2087}|x_{39831})$，我们这里是说在给定了 y 的这样一个条件下，二者才是有条件的独立。）

然后我们就得到了等式：
$$
\begin{aligned}
p(x_1, ..., x_{50000}|y) & = p(x_1|y)p(x_2|y,x_1)p(x_3|y,x_1,x_2) ... p(x_{50000}|y,x_1,x_2,...,,x_{49999})\\
& = p(x_1|y)p(x_2|y)p(x_3|y) ... p(x_{50000}|y)\\
& = \prod^n_{i=1}p(x_i|y)\\
\end{aligned}
$$
第一行的等式就是简单地来自概率的基本性质，第二个等式则使用了朴素贝叶斯假设。

我们这个模型的参数为 $\phi_{i|y=1} = p (x_i = 1|y = 1), \phi_{i|y=0} = p (x_i = 1|y = 0)$, 而 $\phi_y = p (y = 1)$。和以往一样，给定一个训练集$\{(x^{(i)},y^{(i)}); i = 1, ..., m\}$，就可以写出下面的联合似然函数：
$$
L(\phi_y,\phi_{j|y=0},\phi_{j|y=1})=\prod^m_{i=1}p(x^{(i)},y^{(i)})
$$
找到使联合似然函数取得最大值的对应参数组合 $\phi_y$ , $\phi_i|y=0$ 和 $\phi_i|y=1$ 就给出了最大似然估计：
$$
\begin{aligned}
\phi_{j|y=1} &=\frac{\sum^m_{i=1}I(x^{(i)} =1,y^{(i)} =1) }{\sum^m_{i=1}I(y^{(i)} =1)} \\
\phi_{j|y=0} &= \frac{\sum^m_{i=1}I(x^{(i)} =1 , y^{(i)} =0) }{\sum^m_{i=1}1\{y^{(i)} =0\}} \\
\phi_{y} &= \frac{\sum^m_{i=1}I(y^{(i)} =1)}{m}\\
\end{aligned}
$$
这些参数有一个非常自然的解释。例如 $\phi_{j|y=1}$ 正是单词 j 出现的邮件中垃圾邮件所占 $(y = 1)$ 的比例。

拟合好了全部这些参数之后，要对一个新样本的特征向量 x 进行预测，只要进行如下的简单地计算：
$$
\begin{aligned}
p(y=1|x)&=  \frac{p(x|y=1)p(y=1)}{p(x)}\\
&= \frac{(\prod^n_{i=1}p(x_i|y=1))p(y=1)}{(\prod^n_{i=1}p(x_i|y=1))p(y=1)+  (\prod^n_{i=1}p(x_i|y=0))p(y=0)}  \\
\end{aligned}
$$
然后选择有最高后验概率的概率。

# 拉普拉斯光滑（Laplace smoothing）

考虑垃圾邮件分类的过程，若出现一个之前从未出现过的单词，比如ASDFG，单词就从来都没有出现在你的垃圾/正常邮件训练集里面。加入这个 ASDFG 是你字典中的第35000个单词那么你的朴素贝叶斯垃圾邮件筛选器就要对参数$\phi_{35000|y}$ 进行最大似然估计，如下所示：
$$
\begin{aligned}
\phi_{35000|y=1} &=  \frac{\sum^m_{i=1}1\{x^{(i)}_{35000}=1 \wedge y^{(i)}=1  \}}{\sum^m_{i=1}1\{y^{(i)}=0\}}  &=0 \\
\phi_{35000|y=0} &=  \frac{\sum^m_{i=1}1\{x^{(i)}_{35000}=1 \wedge y^{(i)}=0  \}}{\sum^m_{i=1}1\{y^{(i)}=0\}}  &=0 \\
\end{aligned}
$$
也就是说认为看到这个词出现在这两种邮件中的概率都是0。因此当要决定一个包含 ASDFG 这个单词的邮件是否为垃圾邮件的时候，他就检验这个类的后验概率，然后得到了：
$$
\begin{aligned}
p(y=1|x) &= \frac{ \prod^n_{i=1} p(x_i|y=1)p(y=1) }   {\prod^n_{i=1} p(x_i|y=1)p(y=1) +\prod^n_{i=1} p(x_i|y=1)p(y=0)    }\\
&= \frac00\\
\end{aligned}
$$
这是因为对于"  $\prod^n_{i=1} p(x_i|y)$"中包含了$p(x_{35000}|y) = 0$的都加了起来，也就还是0。所以我们的算法得到的就是 $\frac00$，也就是不知道该做出怎么样的预测了。

然后进一步拓展一下这个问题，统计学上来说，只因为你在自己以前的有限的训练数据集中没见到过一件事，就估计这个事件的概率为零，明显是个坏主意。假设问题是估计一个多项式随机变量 z ，其取值范围在$\{1,..., k\}$之内。接下来就可以用$\phi_i = p (z = i)$ 来作为多项式参数。给定一个 m 个独立观测$\{z^{(1)}, ..., z^{(m)}\}$ 组成的集合，然后最大似然估计的形式如下：
$$
\phi_j=\frac{\sum^m_{i=1}1\{z^{(i)}=j\}}m
$$
正如咱们之前见到的，如果我们用这些最大似然估计，那么一些$\phi_j$可能最终就是零了，这就是个问题了。要避免这个情况，咱们就可以引入拉普拉斯光滑（Laplace smoothing），这种方法把上面的估计替换成：
$$
\phi_j=\frac{\sum^m_{i=1}1\{z^{(i)}=j\}+1}{m+k}
$$
这里首先是对分子加1，然后对分母加K，要注意$\sum^k_{j=1} \phi_j = 1$依然成立（自己检验一下），这是一个必须有的性质，因为$\phi_j$ 是对概率的估计，然后所有的概率加到一起必然等于1。另外对于所有的 j 值，都有$\phi_j \neq 0$，这就解决了刚刚的概率估计为零的问题了。在某些特定的条件下（相当强的假设条件下，arguably quite strong），可以发现拉普拉斯光滑还真能给出对参数$\phi_j$ 的最佳估计（optimal estimator）。

回到我们的朴素贝叶斯分选器问题上，使用了拉普拉斯光滑之后，对参数的估计就写成了下面的形式：
$$
\begin{aligned}
\phi_{j=1} & =\frac{\sum^m_{i=1}1\{x_j^{(i)}=1\wedge y ^{(i)}=1\}+1}{\sum^m_{i=1}1{\{y^{(i)}=1\}}+2}\\
\phi_{j=0} & =\frac{\sum^m_{i=1}1\{x_j^{(i)}=1\wedge y ^{(i)}=10\}+1}{\sum^m_{i=1}1{\{y^{(i)}=0\}}+2}\\
\end{aligned}
$$
（在实际应用中，通常是否对$\phi_y$ 使用拉普拉斯并没有太大影响，因为通常我们会对每个垃圾邮件和非垃圾邮件都有一个合适的划分比例，所以$\phi_y$ 会是对$p(y = 1)$ 的一个合理估计，无论如何都会与零点有一定距离。）

# SKlearn使用





